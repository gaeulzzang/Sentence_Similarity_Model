{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 0}, "page_content": "Large Language Model\nThe Llama 4 herd: The beginning of a\nnew era of natively multimodal AI\ninnovation\nApril 5, 2025 •\n12 minute read\nTakeaways\nWe’re sharing the first models in the Llama 4 herd, which will enable people to build more\npersonalized multimodal experiences.\nLlama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal\nmodel in the world in its class and is more powerful than all previous generation Llama", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 0}, "page_content": "models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an\nindustry-leading context window of 10M and delivers better results than Gemma 3, Gemini\n2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.\nLlama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best\nmultimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 0}, "page_content": "of widely reported benchmarks, while achieving comparable results to the new DeepSeek\nv3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 1}, "page_content": "As more people continue to use artificial intelligence to enhance their daily lives, it’s important\nthat the leading models and systems are openly available so everyone can build the future of\npersonalized experiences. Today, we’re excited to announce the most advanced suite of\nmodels that support the entire Llama ecosystem. We’re introducing Llama 4 Scout and Llama\n4 Maverick, the first open-weight natively multimodal models with unprecedented context", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 1}, "page_content": "length support and our first built using a mixture-of-experts (MoE) architecture. We’re also\npreviewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful\nyet to serve as a teacher for our new models.\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We\ndesigned two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion active\nparameter model with 16 experts, and Llama 4 Maverick, a 17 billion active parameter model", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 1}, "page_content": "with 128 experts. The former fits on a single H100 GPU (with Int4 quantization) while the latter\nfits on a single H100 host. We also trained a teacher model, Llama 4 Behemoth, that\noutperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on STEM-focused benchmarks\nsuch as MATH-500 and GPQA Diamond. While we’re not yet releasing Llama 4 Behemoth as\nit is still training, we’re excited to share more technical details about our approach.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 1}, "page_content": "We continue to believe that openness drives innovation and is good for developers, good for\nMeta, and good for the world. We’re making Llama 4 Scout and Llama 4 Maverick available for\ndownload today on llama.com and Hugging Face so everyone can continue to build new\nexperiences using our latest technology. We’ll also make them available via our partners in the\ncoming days. You can also try Meta AI with Llama 4 starting today in WhatsApp, Messenger,\nInstagram Direct, and on the Meta.AI website.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 1}, "page_content": "This is just the beginning for the Llama 4 collection. We believe that the most intelligent\nsystems need to be capable of taking generalized actions, conversing naturally with humans,\nand working through challenging problems they haven’t seen before. Giving Llama\nsuperpowers in these areas will lead to better products for people on our platforms and more\nopportunities for developers to innovate on the next big consumer and business use cases.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 1}, "page_content": "We’re continuing to research and prototype both models and products, and we’ll share more\nabout our vision at LlamaCon on April 29—sign up to hear more.\noffers a best-in-class performance to cost ratio with an experimental chat version scoring\nELO of 1417 on LMArena.\nThese models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion\nactive parameter model with 16 experts that is our most powerful yet and among the", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 1}, "page_content": "world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and\nGemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and\nwe’re excited to share more details about it even while it’s still in flight.\nDownload the Llama 4 Scout and Llama 4 Maverick models today on llama.com and\nHugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct,\nand on the web.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 2}, "page_content": "Whether you’re a developer building on top of our models, an enterprise integrating them into\nyour workflows, or simply curious about the potential uses and benefits of AI, Llama 4 Scout\nand Llama 4 Maverick are the best choices for adding next-generation intelligence to your\nproducts. Today, we’re excited to share more about the four major parts of their development\nand insights into our research and design process. We also can’t wait to see the incredible", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 2}, "page_content": "new experiences the community builds with our new Llama 4 models.\nPre-training\nThese models represent the best of Llama, offering multimodal intelligence at a compelling\nprice while outperforming models of significantly larger sizes. Building the next generation of\nLlama models required us to take several new approaches during pre-training.\nOur new Llama 4 models are our first models that use a mixture of experts (MoE) architecture.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 2}, "page_content": "In MoE models, a single token activates only a fraction of the total parameters. MoE\narchitectures are more compute efficient for training and inference and, given a fixed training\nFLOPs budget, delivers higher quality compared to a dense model.\nAs an example, Llama 4 Maverick models have 17B active parameters and 400B total\nparameters. We use alternating dense and mixture-of-experts (MoE) layers for inference", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 2}, "page_content": "efficiency. MoE layers use 128 routed experts and a shared expert. Each token is sent to the\nshared expert and also to one of the 128 routed experts. As a result, while all parameters are", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 3}, "page_content": "stored in memory, only a subset of the total parameters are activated while serving these\nmodels. This improves inference efficiency by lowering model serving costs and latency—\nLlama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with\ndistributed inference for maximum efficiency.\nLlama 4 models are designed with native multimodality, incorporating early fusion to\nseamlessly integrate text and vision tokens into a unified model backbone. Early fusion is a", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 3}, "page_content": "major step forward, since it enables us to jointly pre-train the model with large amounts of\nunlabeled text, image, and video data. We also improved the vision encoder in Llama 4. This is\nbased on MetaCLIP but trained separately in conjunction with a frozen Llama model to better\nadapt the encoder to the LLM.\nWe developed a new training technique which we refer to as MetaP that allows us to reliably\nset critical model hyper-parameters such as per-layer learning rates and initialization scales.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 3}, "page_content": "We found that chosen hyper-parameters transfer well across different values of batch size,\nmodel width, depth, and training tokens. Llama 4 enables open source fine-tuning efforts by\npre-training on 200 languages, including over 100 with over 1 billion tokens each, and overall\n10x more multilingual tokens than Llama 3.\nAdditionally, we focus on efficient model training by using FP8 precision, without sacrificing", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 3}, "page_content": "quality and ensuring high model FLOPs utilization—while pre-training our Llama 4 Behemoth\nmodel using FP8 and 32K GPUs, we achieved 390 TFLOPs/GPU. The overall data mixture for\ntraining consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-\ntraining mixture and includes diverse text, image, and video datasets.\nWe continued training the model in what we call “mid-training” to improve core capabilities with", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 3}, "page_content": "new training recipes including long context extension using specialized datasets. This enabled\nus to enhance model quality while also unlocking best-in-class 10M input context length for\nLlama 4 Scout.\nPost-training our new models\nOur newest models include smaller and larger options to accommodate a range of use cases\nand developer needs. Llama 4 Maverick offers unparalleled, industry-leading performance in", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 3}, "page_content": "image and text understanding, enabling the creation of sophisticated AI applications that\nbridge language barriers. As our product workhorse model for general assistant and chat use\ncases, Llama 4 Maverick is great for precise image understanding and creative writing.\nThe biggest challenge while post-training the Llama 4 Maverick model was maintaining a\nbalance between multiple input modalities, reasoning, and conversational abilities. For mixing", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 3}, "page_content": "modalities, we came up with a carefully curated curriculum strategy that does not trade-off\nperformance compared to the individual modality expert models. With Llama 4, we revamped\nour post-training pipeline by adopting a different approach: lightweight supervised fine-tuning\n(SFT) > online reinforcement learning (RL) > lightweight direct preference optimization (DPO).\nA key learning was that SFT and DPO can over-constrain the model, restricting exploration", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 4}, "page_content": "during the online RL stage and leading to suboptimal accuracy, particularly in reasoning,\ncoding, and math domains. To address this, we removed more than 50% of our data tagged as\neasy by using Llama models as a judge and did lightweight SFT on the remaining harder set.\nIn the subsequent multimodal online RL stage, by carefully selecting harder prompts, we were\nable to achieve a step change in performance. Furthermore, we implemented a continuous", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 4}, "page_content": "online RL strategy, where we alternated between training the model and then using it to\ncontinually filter and retain only medium-to-hard difficulty prompts. This strategy proved highly\nbeneficial in terms of compute and accuracy tradeoffs. We then did a lightweight DPO to\nhandle corner cases related to model response quality, effectively achieving a good balance\nbetween the model’s intelligence and conversational abilities. Both the pipeline architecture", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 4}, "page_content": "and the continuous online RL strategy with adaptive data filtering culminated in an industry-\nleading, general-purpose chat model with state-of-the-art intelligence and image\nunderstanding capabilities.\nAs a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128\nexperts, and 400 billion total parameters, offering high quality at a lower price compared to\nLlama 3.3 70B. Llama 4 Maverick is the best-in-class multimodal model, exceeding", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 4}, "page_content": "comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-\ncontext, and image benchmarks, and it’s competitive with the much larger DeepSeek v3.1 on\ncoding and reasoning.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 5}, "page_content": "Our smaller model, Llama 4 Scout, is a general purpose model with 17 billion active\nparameters, 16 experts, and 109 billion total parameters that delivers state-of-the-art\nperformance for its class. Llama 4 Scout dramatically increases the supported context length\nfrom 128K in Llama 3 to an industry leading 10 million tokens. This opens up a world of\npossibilities, including multi-document summarization, parsing extensive user activity for\npersonalized tasks, and reasoning over vast codebases.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 5}, "page_content": "Llama 4 Scout is both pre-trained and post-trained with a 256K context length, which\nempowers the base model with advanced length generalization capability. We present\ncompelling results in tasks such as retrieval with “retrieval needle in haystack” for text as well\nas cumulative negative log-likelihoods (NLLs) over 10 million tokens of code. A key innovation\nin the Llama 4 architecture is the use of interleaved attention layers without positional", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 5}, "page_content": "embeddings. Additionally, we employ inference time temperature scaling of attention to\nenhance length generalization. We call this the iRoPE architecture, where “i” stands for\n“interleaved” attention layers, highlighting the long-term goal of supporting “infinite” context\nlength, and “RoPE” refers to the rotary position embeddings employed in most layers.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 6}, "page_content": "0:00 / 0:18", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 7}, "page_content": "We trained both of our models on a wide variety of image and video frame stills in order to give\nthem broad visual understanding, including of temporal activities and related images. This\nenables effortless interaction on multi-image inputs alongside text prompts for visual reasoning\nand understanding tasks. The models were pre-trained on up to 48 images, and we’ve tested\nin post-training with good results up to eight images.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/llama4.pdf", "page": 7}, "page_content": "Llama 4 Scout is also best-in-class on image grounding, able to align user prompts with\nrelevant visual concepts and anchor model responses to regions in the image. This enables\nmore precise visual question answering for the LLM to better understand user intent and\nlocalize objects of interest. Llama 4 Scout also exceeds comparable models on coding,\nreasoning, long context, and image benchmarks and offers stronger performance than all\nprevious Llama models.", "type": "Document"}
