{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 0}, "page_content": "Qwen3: Think Deeper, Act Faster\nApril 29, 2025 · 10 min · 2036 words · Qwen Team | Translations: 简体中文\n \n \n \n \n \nIntroduction\nToday, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language\nmodels. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of\ncoding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 0}, "page_content": "mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B\nwith 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of\nQwen2.5-72B-Instruct.\nQWEN CHAT ↗\nGITHUB ↗\nHUGGING FACE ↗\nMODELSCOPE ↗\nKAGGLE ↗\nDEMO ↗\nDISCORD ↗\nBlog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 1}, "page_content": "We are open-weighting two MoE models: Qwen3-235B-A22B, a large model with 235 billion total parameters and\n22 billion activated parameters, and Qwen3-30B-A3B, a smaller MoE model with 30 billion total parameters and\n3 billion activated parameters. Additionally, six dense models are also open-weighted, including Qwen3-32B,\nQwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B, under Apache 2.0 license.\nModels\nLayers\nHeads (Q / KV)\nTie Embedding\nContext Length\nQwen3-0.6B\n28\n16 / 8\nYes\n32K\nBlog", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 1}, "page_content": "Qwen3-0.6B\n28\n16 / 8\nYes\n32K\nBlog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 2}, "page_content": "Models\nLayers\nHeads (Q / KV)\nTie Embedding\nContext Length\nQwen3-1.7B\n28\n16 / 8\nYes\n32K\nQwen3-4B\n36\n32 / 8\nYes\n32K\nQwen3-8B\n36\n32 / 8\nNo\n128K\nQwen3-14B\n40\n40 / 8\nNo\n128K\nQwen3-32B\n64\n64 / 8\nNo\n128K\nModels\nLayers\nHeads (Q / KV)\n# Experts (Total / Activated)\nContext Length\nQwen3-30B-A3B\n48\n32 / 4\n128 / 8\n128K\nQwen3-235B-A22B\n94\n64 / 4\n128 / 8\n128K\nThe post-trained models, such as Qwen3-30B-A3B, along with their pre-trained counterparts (e.g., Qwen3-30B-", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 2}, "page_content": "A3B-Base), are now available on platforms like Hugging Face, ModelScope, and Kaggle. For deployment, we\nrecommend using frameworks like SGLang and vLLM. For local usage, tools such as Ollama, LMStudio, MLX,\nllama.cpp, and KTransformers are highly recommended. These options ensure that users can easily integrate\nQwen3 into their workflows, whether in research, development, or production environments.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 2}, "page_content": "We believe that the release and open-sourcing of Qwen3 will significantly advance the research and development\nof large foundation models. Our goal is to empower researchers, developers, and organizations around the world\nto build innovative solutions using these cutting-edge models.\nFeel free to try Qwen3 out in Qwen Chat Web (chat.qwen.ai) and mobile APP!\nKey Features\nHybrid Thinking Modes\nQwen3 models introduce a hybrid approach to problem-solving. They support two modes:", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 2}, "page_content": "1. Thinking Mode: In this mode, the model takes time to reason step by step before delivering the final answer.\nThis is ideal for complex problems that require deeper thought.\n2. Non-Thinking Mode: Here, the model provides quick, near-instant responses, suitable for simpler questions\nwhere speed is more important than depth.\nThis flexibility allows users to control how much “thinking” the model performs based on the task at hand. For", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 2}, "page_content": "example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly\nwithout delay. Crucially, the integration of these two modes greatly enhances the model’s ability to implement\nstable and efficient thinking budget control. As demonstrated above, Qwen3 exhibits scalable and smooth\nperformance improvements that are directly correlated with the computational reasoning budget allocated. This\nBlog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 3}, "page_content": "design enables users to configure task-specific budgets with greater ease, achieving a more optimal balance\nbetween cost efficiency and inference quality.\nMultilingual Support\nQwen3 models are supporting 119 languages and dialects. This extensive multilingual capability opens up new\npossibilities for international applications, enabling users worldwide to benefit from the power of these models.\nLanguage\nFamily\nLanguages & Dialects\nIndo-\nEuropean", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 3}, "page_content": "Languages & Dialects\nIndo-\nEuropean\nEnglish, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech,\nGreek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål,\nNorwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi,\nBelarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian,\nChhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya,", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 3}, "page_content": "Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan,\nIcelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi,\nBosnian, Armenian\nSino-Tibetan\nChinese (Simplified Chinese, Traditional Chinese, Cantonese), Burmese\nAfro-Asiatic\nArabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta’izzi-Adeni, Tunisian),\nHebrew, Maltese\nAustronesian", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 3}, "page_content": "Hebrew, Maltese\nAustronesian\nIndonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar,\nPangasinan, Iloko, Waray (Philippines)\nBlog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 4}, "page_content": "Language\nFamily\nLanguages & Dialects\nDravidian\nTamil, Telugu, Kannada, Malayalam\nTurkic\nTurkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar\nTai-Kadai\nThai, Lao\nUralic\nFinnish, Estonian, Hungarian\nAustroasiatic\nVietnamese, Khmer\nOther\nJapanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili\nImproved Agentic Capabilities\nWe have optimized the Qwen3 models for coding and agentic capabilities, and also we have strengthened the", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 4}, "page_content": "support of MCP as well. Below we provide examples to show how Qwen3 thinks and interacts with the\nenvironment.\nPre-training\nIn terms of pretraining, the dataset for Qwen3 has been significantly expanded compared to Qwen2.5. While\nQwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36\n0:00 / 0:55\nBlog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 5}, "page_content": "trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from\nthe web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and\nQwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used\nQwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs,\nand code snippets.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 5}, "page_content": "and code snippets.\nThe pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30\ntrillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and\ngeneral knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of\nknowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 5}, "page_content": "additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length\nto 32K tokens. This ensures the model can handle longer inputs effectively.\nDue to advancements in model architecture, increase in training data, and more effective training methods, the\noverall performance of Qwen3 dense base models matches that of Qwen2.5 base models with more parameters.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 5}, "page_content": "For instance, Qwen3-1.7B/4B/8B/14B/32B-Base performs as well as Qwen2.5-3B/7B/14B/32B/72B-Base,\nrespectively. Notably, in areas like STEM, coding, and reasoning, Qwen3 dense base models even outperform\nlarger Qwen2.5 models. For Qwen3-MoE base models, they achieve similar performance to Qwen2.5 dense base\nmodels while using only 10% of the active parameters. This results in significant savings in both training and\ninference costs.\nBlog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 6}, "page_content": "Post-training\nTo develop the hybrid model capable of both step-by-step reasoning and rapid responses, we implemented a\nfour-stage training pipeline. This pipeline includes: (1) long chain-of-thought (CoT) cold start, (2) reasoning-based\nreinforcement learning (RL), (3) thinking mode fusion, and (4) general RL.\nIn the first stage, we fine-tuned the models using diverse long CoT data, covering various tasks and domains", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 6}, "page_content": "such as mathematics, coding, logical reasoning, and STEM problems. This process aimed to equip the model\nwith fundamental reasoning abilities. The second stage focused on scaling up computational resources for RL,\nutilizing rule-based rewards to enhance the model’s exploration and exploitation capabilities.\nIn the third stage, we integrated non-thinking capabilities into the thinking model by fine-tuning it on a", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 6}, "page_content": "combination of long CoT data and commonly used instruction-tuning data. This data was generated by the\nenhanced thinking model from the second stage, ensuring a seamless blend of reasoning and quick response\ncapabilities. Finally, in the fourth stage, we applied RL across more than 20 general-domain tasks to further\nstrengthen the model’s general capabilities and correct undesired behaviors. These tasks included instruction\nfollowing, format following, and agent capabilities, etc.", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 6}, "page_content": "Develop with Qwen3\nBelow is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard\nexample of using Qwen3-30B-A3B in Hugging Face transformers:\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-30B-A3B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\nBlog\nPublication\nAbout", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 6}, "page_content": "Blog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 7}, "page_content": "To disable thinking, you just need to make changes to the argument enable_thinking  like the following:\nFor deployment, you can use sglang>=0.4.6.post1  or vllm>=0.8.4  to create an OpenAI-compatible API\nendpoint:\nSGLang:\nvLLM:\nIf you use it for local development, you can use ollama by running a simple command ollama run qwen3:30b-\na3b  to play with the model, or you can use LMStudio or llama.cpp and ktransformers to build locally.\nAdvanced Usages\n)\n# prepare the model input", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 7}, "page_content": ")\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 7}, "page_content": "**model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")", "type": "Document"}
{"id": null, "metadata": {"file_path": "assets/pdf_tmp/qwen3.pdf", "page": 7}, "page_content": "print(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # True is the default value for enable_thinking.\n)\npython -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3\nvllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1\nBlog\nPublication\nAbout\nTry Qwen Chat", "type": "Document"}
